{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Grammars: A Grammar-Induction Based Method for Learning Temporal Abstractions\n",
    "## Authors: Robert Lange and Aldo Faisal | April 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_hanoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.q_agent import Agent_Q\n",
    "from agents.smdp_q_agent import SMDP_Agent_Q, Macro, SMDPQTable\n",
    "# from agents.a2c_agent import ActorCritic, train_a2c_agent\n",
    "\n",
    "from learning.q_learning import  q_learning\n",
    "from learning.smdp_q_learning import smdp_q_learning, smdp_q_online_learning\n",
    "\n",
    "from learning.learning_params import *\n",
    "from learning.run_learning import *\n",
    "\n",
    "from utils.general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created New Log Directory\n"
     ]
    }
   ],
   "source": [
    "# Create directory - Learning performance\n",
    "results_dir = os.getcwd() + \"/results/\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(\"Created New Results Directory\")\n",
    "    \n",
    "# Create directory - Figure directory\n",
    "fig_dir = os.getcwd() + \"/figures/\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    print(\"Created New Fig Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towers of Hanoi - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.1, 'epsilon': 0.1}\n",
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.0, 'epsilon': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(learning_parameters(\"Q-Learning\"))\n",
    "print(learning_parameters(\"Imitation-SMDP-Q-Learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_setup = {4: {\"num_episodes\": 750,\n",
    "                      \"max_steps\": 500},\n",
    "                  5: {\"num_episodes\": 5000,\n",
    "                      \"max_steps\": 2000},\n",
    "                  6: {\"num_episodes\": 10000,\n",
    "                      \"max_steps\": 5000},\n",
    "                  7: {\"num_episodes\": 10000,\n",
    "                      \"max_steps\": 4000},\n",
    "                  8: {\"num_episodes\": 20000,\n",
    "                      \"max_steps\": 8000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple TD($\\lambda$) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for N=4 Disk Towers of Hanoi Environment\n",
    "log_episodes = 1\n",
    "log_freq = 5\n",
    "\n",
    "N = 4\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep:  6 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 11 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 16 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 21 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 26 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 31 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 36 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 41 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 46 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 51 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 56 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 61 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 66 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 71 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 76 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 86 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 91 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 96 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 101 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 106 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 111 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 116 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 121 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 126 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 131 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 136 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 141 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 146 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 151 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 156 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 161 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 166 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 171 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 176 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 181 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 186 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 191 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 196 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 201 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 206 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 211 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 216 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 221 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 226 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 231 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 236 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 241 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 246 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 251 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 256 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 261 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 266 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 271 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 276 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 281 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 286 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 291 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 296 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 301 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 306 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 311 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 316 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 321 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 326 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 331 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 336 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 341 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 346 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 351 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 356 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 361 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 366 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 371 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 376 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 381 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 386 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 391 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 396 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 401 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 406 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 411 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 416 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 421 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 426 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 431 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 436 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 441 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 446 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 451 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 456 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 461 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 466 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 471 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 476 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 481 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 486 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 491 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 496 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 501 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 506 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 511 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 516 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 521 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 526 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 531 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 536 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 541 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 546 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 551 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 556 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 561 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 566 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 571 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 576 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 581 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 586 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 591 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 596 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 601 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 606 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 611 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 616 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 621 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 626 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 631 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 636 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 641 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 646 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 651 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 656 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 661 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 666 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 671 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 676 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 681 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 686 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 691 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 696 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 701 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 706 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 711 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 716 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 721 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 726 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 731 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 736 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 741 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 746 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "agent = Agent_Q(env)\n",
    "params = learning_parameters(l_type=\"Q-Learning\")\n",
    "hist, er_buffer = q_learning(agent, num_episodes, max_steps,\n",
    "                             **params, log_freq=log_freq,\n",
    "                             log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_4_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                         num_episodes, max_steps,\n",
    "                         log_episodes, log_freq,\n",
    "                         save_fname=\"results/4_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_5_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                         num_episodes, max_steps,\n",
    "                         log_episodes, log_freq,\n",
    "                         save_fname=\"results/5_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Disks - Q-Learning: Run 1/5 Done - Time: 2148.86\n",
      "6 Disks - Q-Learning: Run 2/5 Done - Time: 9221.74\n",
      "6 Disks - Q-Learning: Run 3/5 Done - Time: 10751.56\n",
      "6 Disks - Q-Learning: Run 4/5 Done - Time: 10833.3\n",
      "6 Disks - Q-Learning: Run 5/5 Done - Time: 10875.95\n",
      "Outfiled the results to results/6_disks_q.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_6_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                         num_episodes, max_steps,\n",
    "                         log_episodes, log_freq,\n",
    "                         save_fname=\"results/6_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return a greedy rollout Experience Replay Episode\n",
    "get_rollout_policy(env, agent, max_steps, grammar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_rollout_policy(env, agent, max_steps, grammar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Context-Free Grammar Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policies = {4: \"abdaefabdcedabd\",\n",
    "                    5: \"bafbcdbafecfbafbcdbcfecdbafbcdb\",\n",
    "                    6: \"abdaefabdcedabdaefaedcefabdaefabdcedabdce\"\\\n",
    "                        \"faedcedabdaefabdcedabd\",\n",
    "                    7: \"bafbcdbafecfbafbcdbcfecdbafbcdbafecfbafec\"\\\n",
    "                        \"dbcfecfbafbcdbafecfbafbcdbcfecdbafbcdbcfe\"\\\n",
    "                        \"cfbafecdbcfecdbafbcdbafecfbafbcdbcfecdbafbcdb\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammars.cfg_grammar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_macros(\"all\", optimal_policies[4], 6, \"sequitur\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "get_macros(\"all\", optimal_policies[4], 6, \"lexis\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitation SMDP-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 4\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)\n",
    "\n",
    "macros = get_optimal_macros(env, N, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "params = learning_parameters(l_type=\"Imitation-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_4_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                num_episodes, max_steps,\n",
    "                                log_episodes, log_freq,\n",
    "                                save_fname=\"results/4_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_5_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                num_episodes, max_steps,\n",
    "                                log_episodes, log_freq,\n",
    "                                save_fname=\"results/5_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Disks - Imitation-SMDP-Q-Learning: Run 1/5 Done - Time: 40.36\n",
      "6 Disks - Imitation-SMDP-Q-Learning: Run 2/5 Done - Time: 40.92\n",
      "6 Disks - Imitation-SMDP-Q-Learning: Run 3/5 Done - Time: 42.26\n",
      "6 Disks - Imitation-SMDP-Q-Learning: Run 4/5 Done - Time: 39.79\n",
      "6 Disks - Imitation-SMDP-Q-Learning: Run 5/5 Done - Time: 42.83\n",
      "Outfiled the results to results/6_disks_smdp_imi.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_6_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                num_episodes, max_steps,\n",
    "                                log_episodes, log_freq,\n",
    "                                save_fname=\"results/6_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 1\n",
    "log_freq = 5\n",
    "\n",
    "N = 5\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macros = get_optimal_macros(env, N-1, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Transfer-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment with 4 disk grammar\n",
    "num_times = 5\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "stats_smdp_trans_5 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                  num_episodes, max_steps,\n",
    "                                  log_episodes, log_freq,\n",
    "                                  transfer_distance=1,\n",
    "                                  save_fname=\"results/5_disks_smdp_transfer_4_disks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6 Disks - Transfer-SMDP-Q-Learning: Run 1/5 Done - Time: 92.58\n",
      "6 Disks - Transfer-SMDP-Q-Learning: Run 2/5 Done - Time: 95.87\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-eb5e7ace8981>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m                                   \u001b[0mlog_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m                                   \u001b[0mtransfer_distance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m                                   save_fname=\"results/6_disks_smdp_transfer_5_disks.txt\")\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# Run Learning 5 times for 6 Disk Environment with 4 disk grammar\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/learning/run_learning.py\u001b[0m in \u001b[0;36mrun_learning\u001b[0;34m(l_type, num_times, num_disks, num_episodes, max_steps, log_episodes, log_freq, transfer_distance, save_fname)\u001b[0m\n\u001b[1;32m     57\u001b[0m                                               \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m                                               \u001b[0mlog_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m                                               verbose=False)\n\u001b[0m\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ml_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Online-SMDP-Q-Learning\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/learning/smdp_q_learning.py\u001b[0m in \u001b[0;36msmdp_q_learning\u001b[0;34m(env, agent, num_episodes, max_steps, gamma, alpha, lambd, epsilon, log_freq, log_episodes, verbose)\u001b[0m\n\u001b[1;32m     72\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m                 \u001b[0mreward\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mgreedy_choice\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0;31m# Update value function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/agents/q_agent.py\u001b[0m in \u001b[0;36mgreedy_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/AG/lib/python3.6/site-packages/numpy-1.16.0rc2-py3.6-macosx-10.7-x86_64.egg/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36mamax\u001b[0;34m(a, axis, out, keepdims, initial)\u001b[0m\n\u001b[1;32m   2503\u001b[0m     \"\"\"\n\u001b[1;32m   2504\u001b[0m     return _wrapreduction(a, np.maximum, 'max', axis, None, out, keepdims=keepdims,\n\u001b[0;32m-> 2505\u001b[0;31m                           initial=initial)\n\u001b[0m\u001b[1;32m   2506\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2507\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda2/envs/AG/lib/python3.6/site-packages/numpy-1.16.0rc2-py3.6-macosx-10.7-x86_64.egg/numpy/core/fromnumeric.py\u001b[0m in \u001b[0;36m_wrapreduction\u001b[0;34m(obj, ufunc, method, axis, dtype, out, **kwargs)\u001b[0m\n\u001b[1;32m     84\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mreduction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mufunc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpasskwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_times = 5\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "\n",
    "# Run Learning 5 times for 6 Disk Environment with 5 disk grammar\n",
    "stats_smdp_trans_5 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                  num_episodes, max_steps,\n",
    "                                  log_episodes, log_freq,\n",
    "                                  transfer_distance=1,\n",
    "                                  save_fname=\"results/6_disks_smdp_transfer_5_disks.txt\")\n",
    "\n",
    "# Run Learning 5 times for 6 Disk Environment with 4 disk grammar\n",
    "stats_smdp_trans_4 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                  num_episodes, max_steps,\n",
    "                                  log_episodes, log_freq,\n",
    "                                  transfer_distance=2,\n",
    "                                  save_fname=\"results/6_disks_smdp_transfer_4_disks.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online-Grammar-Macro-SMDP Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup for N=4 Disk Towers of Hanoi Environment\n",
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "init_q_eps = 20\n",
    "inter_update_eps=100\n",
    "num_grammar_updates=5\n",
    "\n",
    "N = 4\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(N, env_noise=0, verbose=False)\n",
    "\n",
    "params = learning_parameters(l_type=\"Q-Learning\")\n",
    "hist = smdp_q_online_learning(env, init_q_eps, inter_update_eps,\n",
    "                              num_grammar_updates, max_steps, **params,\n",
    "                              log_freq=log_freq, log_episodes=log_episodes,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 4\n",
    "stats_smdp_online = run_learning(\"Online-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                 num_episodes, max_steps,\n",
    "                                 log_episodes, log_freq,\n",
    "                                 save_fname=\"results/4_disks_smdp_online_no_transfer.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (AG)",
   "language": "python",
   "name": "ag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
