{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Grammars: A Grammar-Induction Based Method for Learning Temporally-Extended Actions\n",
    "## Authors: Robert Lange and Aldo Faisal | January 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_hanoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.q_agent import Agent_Q\n",
    "from agents.smdp_q_agent import SMDP_Agent_Q, Macro, SMDPQTable\n",
    "\n",
    "from learning.q_learning import  q_learning\n",
    "from learning.smdp_q_learning import smdp_q_learning\n",
    "\n",
    "from learning.learning_params import *\n",
    "from learning.run_learning import *\n",
    "\n",
    "from utils.general import *\n",
    "from utils.plotting import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory - Learning performance\n",
    "results_dir = os.getcwd() + \"/results/\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(\"Created New Results Directory\")\n",
    "\n",
    "# Create directory - Log directory\n",
    "log_dir = os.getcwd() + \"/logs/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(\"Created New Log Directory\")\n",
    "    \n",
    "# Create directory - Figure directory\n",
    "fig_dir = os.getcwd() + \"/figures/\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    print(\"Created New Fig Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towers of Hanoi - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.1, 'epsilon': 0.1}\n",
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.0, 'epsilon': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(learning_parameters(\"Q-Learning\"))\n",
    "print(learning_parameters(\"Imitation-SMDP-Q-Learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_setup = {4: {\"num_episodes\": 300,\n",
    "                      \"max_steps\": 500},\n",
    "                  5: {\"num_episodes\": 1250,\n",
    "                      \"max_steps\": 1000},\n",
    "                  6: {\"num_episodes\": 5000,\n",
    "                      \"max_steps\": 2000},\n",
    "                  7: {\"num_episodes\": 10000,\n",
    "                      \"max_steps\": 4000},\n",
    "                  8: {\"num_episodes\": 20000,\n",
    "                      \"max_steps\": 8000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple TD($\\lambda$) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rtl/anaconda2/envs/AG/lib/python3.6/site-packages/gym-0.10.9-py3.6.egg/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Setup for N=4 Disk Towers of Hanoi Environment\n",
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 4\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 21 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 41 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 61 | Avg/Std Steps: 27.00/0.00 | Avg/Std Ret: 26.35/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 26.00/0.00 | Avg/Std Ret: 27.74/0.00 | Success R: 1.00\n",
      "Ep: 101 | Avg/Std Steps: 23.00/0.00 | Avg/Std Ret: 32.35/0.00 | Success R: 1.00\n",
      "Ep: 121 | Avg/Std Steps: 26.00/0.00 | Avg/Std Ret: 27.74/0.00 | Success R: 1.00\n",
      "Ep: 141 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 161 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 181 | Avg/Std Steps: 23.00/0.00 | Avg/Std Ret: 32.35/0.00 | Success R: 1.00\n",
      "Ep: 201 | Avg/Std Steps: 23.00/0.00 | Avg/Std Ret: 32.35/0.00 | Success R: 1.00\n",
      "Ep: 221 | Avg/Std Steps: 23.00/0.00 | Avg/Std Ret: 32.35/0.00 | Success R: 1.00\n",
      "Ep: 241 | Avg/Std Steps: 23.00/0.00 | Avg/Std Ret: 32.35/0.00 | Success R: 1.00\n",
      "Ep: 261 | Avg/Std Steps: 21.00/0.00 | Avg/Std Ret: 35.85/0.00 | Success R: 1.00\n",
      "Ep: 281 | Avg/Std Steps: 21.00/0.00 | Avg/Std Ret: 35.85/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "agent = Agent_Q(env)\n",
    "params = learning_parameters(l_type=\"Q-Learning\")\n",
    "hist = q_learning(env, agent, num_episodes, max_steps,\n",
    "                  **params, log_freq=log_freq,\n",
    "                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Disks - Q-Learning: Run 1/5 Done - Time: 1.41\n",
      "4 Disks - Q-Learning: Run 2/5 Done - Time: 1.15\n",
      "4 Disks - Q-Learning: Run 3/5 Done - Time: 1.13\n",
      "4 Disks - Q-Learning: Run 4/5 Done - Time: 2.41\n",
      "4 Disks - Q-Learning: Run 5/5 Done - Time: 0.88\n",
      "Outfiled the results to results/4_disks_q.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_4_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/4_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Disks - Q-Learning: Run 1/5 Done - Time: 16.61\n",
      "5 Disks - Q-Learning: Run 2/5 Done - Time: 20.75\n",
      "5 Disks - Q-Learning: Run 3/5 Done - Time: 17.17\n",
      "5 Disks - Q-Learning: Run 4/5 Done - Time: 16.8\n",
      "5 Disks - Q-Learning: Run 5/5 Done - Time: 26.66\n",
      "Outfiled the results to results/5_disks_q.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_5_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/5_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_6_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/6_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(0, (0, 0, 0, 0, 0), 1, (2, 0, 0, 0, 0)),\n",
       "       (1, (2, 0, 0, 0, 0), 0, (2, 1, 0, 0, 0)),\n",
       "       (2, (2, 1, 0, 0, 0), 5, (1, 1, 0, 0, 0)),\n",
       "       (3, (1, 1, 0, 0, 0), 1, (1, 1, 2, 0, 0)),\n",
       "       (4, (1, 1, 2, 0, 0), 2, (0, 1, 2, 0, 0)),\n",
       "       (5, (0, 1, 2, 0, 0), 3, (0, 2, 2, 0, 0)),\n",
       "       (6, (0, 2, 2, 0, 0), 1, (2, 2, 2, 0, 0)),\n",
       "       (7, (2, 2, 2, 0, 0), 0, (2, 2, 2, 1, 0)),\n",
       "       (8, (2, 2, 2, 1, 0), 5, (1, 2, 2, 1, 0)),\n",
       "       (9, (1, 2, 2, 1, 0), 4, (1, 0, 2, 1, 0)),\n",
       "       (10, (1, 0, 2, 1, 0), 2, (0, 0, 2, 1, 0)),\n",
       "       (11, (0, 0, 2, 1, 0), 5, (0, 0, 1, 1, 0)),\n",
       "       (12, (0, 0, 1, 1, 0), 1, (2, 0, 1, 1, 0)),\n",
       "       (13, (2, 0, 1, 1, 0), 0, (2, 1, 1, 1, 0)),\n",
       "       (14, (2, 1, 1, 1, 0), 5, (1, 1, 1, 1, 0)),\n",
       "       (15, (1, 1, 1, 1, 0), 1, (1, 1, 1, 1, 2)),\n",
       "       (16, (1, 1, 1, 1, 2), 2, (0, 1, 1, 1, 2)),\n",
       "       (17, (0, 1, 1, 1, 2), 3, (0, 2, 1, 1, 2)),\n",
       "       (18, (0, 2, 1, 1, 2), 1, (2, 2, 1, 1, 2)),\n",
       "       (19, (2, 2, 1, 1, 2), 2, (2, 2, 0, 1, 2)),\n",
       "       (20, (2, 2, 0, 1, 2), 4, (0, 2, 0, 1, 2)),\n",
       "       (21, (0, 2, 0, 1, 2), 5, (0, 1, 0, 1, 2)),\n",
       "       (22, (0, 1, 0, 1, 2), 1, (2, 1, 0, 1, 2)),\n",
       "       (23, (2, 1, 0, 1, 2), 2, (2, 0, 0, 1, 2)),\n",
       "       (24, (2, 0, 0, 1, 2), 4, (0, 0, 0, 1, 2)),\n",
       "       (25, (0, 0, 0, 1, 2), 3, (0, 0, 0, 2, 2)),\n",
       "       (26, (0, 0, 0, 2, 2), 0, (1, 0, 0, 2, 2)),\n",
       "       (27, (1, 0, 0, 2, 2), 1, (1, 2, 0, 2, 2)),\n",
       "       (28, (1, 2, 0, 2, 2), 3, (2, 2, 0, 2, 2)),\n",
       "       (29, (2, 2, 0, 2, 2), 0, (2, 2, 1, 2, 2)),\n",
       "       (30, (2, 2, 1, 2, 2), 5, (1, 2, 1, 2, 2)),\n",
       "       (31, (1, 2, 1, 2, 2), 4, (1, 0, 1, 2, 2)),\n",
       "       (32, (1, 0, 1, 2, 2), 2, (0, 0, 1, 2, 2)),\n",
       "       (33, (0, 0, 1, 2, 2), 3, (0, 0, 2, 2, 2)),\n",
       "       (34, (0, 0, 2, 2, 2), 0, (1, 0, 2, 2, 2)),\n",
       "       (35, (1, 0, 2, 2, 2), 1, (1, 2, 2, 2, 2)),\n",
       "       (36, (1, 2, 2, 2, 2), 3, (2, 2, 2, 2, 2))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a greedy rollout Experience Replay Episode\n",
    "get_rollout_policy(env, agent, max_steps, grammar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bafbcdbafecfbafbcdbcefbcedabdafecdabd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rollout_policy(env, agent, max_steps, grammar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Context-Free Grammar Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policies = {4: \"abdaefabdcedabd\",\n",
    "                    5: \"bafbcdbafecfbafbcdbcfecdbafbcdb\",\n",
    "                    6: \"abdaefabdcedabdaefaedcefabdaefabdcedabdce\"\\\n",
    "                        \"faedcedabdaefabdcedabd\",\n",
    "                    7: \"bafbcdbafecfbafbcdbcfecdbafbcdbafecfbafec\"\\\n",
    "                        \"dbcfecfbafbcdbafecfbafbcdbcfecdbafbcdbcfe\"\\\n",
    "                        \"cfbafecdbcfecdbafbcdbafecfbafbcdbcfecdbafbcdb\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammars.cfg_grammar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 a e f 1 c e d 1 \\\\n ', 'a b d ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_macros(\"all\", optimal_policies[4], 6, \"sequitur\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_macros(\"all\", optimal_policies[4], 6, \"lexis\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitation SMDP-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 4\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 182.86/96.46 | Avg/Std Ret: 0.28/96.46 | Success R: 0.70\n",
      "Ep: 21 | Avg/Std Steps: 146.33/103.63 | Avg/Std Ret: 4.00/103.63 | Success R: 0.30\n",
      "Ep: 41 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 101 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 121 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 141 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 161 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 181 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 201 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 221 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 241 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 261 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 281 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "macros = get_optimal_macros(env, N, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Imitation-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Disks - Imitation-SMDP-Q-Learning: Run 1/5 Done - Time: 1.3\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 2/5 Done - Time: 1.07\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 3/5 Done - Time: 1.12\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 4/5 Done - Time: 1.39\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 5/5 Done - Time: 1.32\n",
      "Outfiled the results to results/4_disks_smdp_imi.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_4_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/4_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Disks - Imitation-SMDP-Q-Learning: Run 1/5 Done - Time: 23.63\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 2/5 Done - Time: 21.87\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 3/5 Done - Time: 23.22\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 4/5 Done - Time: 21.47\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 5/5 Done - Time: 23.14\n",
      "Outfiled the results to results/5_disks_smdp_imi.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_5_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/5_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_6_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/6_disks_smdp_imi.txt\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 6\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macros = get_optimal_macros(env, N-1, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Transfer-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "macros = get_optimal_macros(env, N-2, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Transfer-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment with 5 disk grammar\n",
    "num_times = 5\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, its_6, steps_6_smdp_trans_5, sd_steps_6_smdp_trans_5, rew_6_smdp_trans_5, sd_rew_6_smdp_trans_5 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                                                       num_episodes, max_steps,\n",
    "                                                                       log_episodes, log_freq, transfer_distance=1, save_fname=\"results/6_disks_smdp_transfer_5_disks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment with 4 disk grammar\n",
    "env, agent, its_6, steps_6_smdp_trans_4, sd_steps_6_smdp_trans_4, rew_6_smdp_trans_4, sd_rew_6_smdp_trans_4 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                                                       num_episodes, max_steps,\n",
    "                                                                       log_episodes, log_freq, transfer_distance=2, save_fname=\"results/6_disks_smdp_transfer_4_disks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "steps_6 = [steps_6_q, steps_6_smdp_trans_5, steps_6_smdp_trans4]\n",
    "sd_steps_6 = [sd_steps_6_q, sd_steps_6_smdp_trans_5, sd_steps_6_smdp_trans_4]\n",
    "plot_learning(its_6, steps_6, sd_steps_6, 1, \"6 Disks ToH - Expert Grammar Transfer Learning\",\n",
    "              label_temp=[r\"Base TD($\\lambda$)\", \"5 Disk Grammar\", \"4 Disk Grammar\"],\n",
    "              save_fname=\"figures/6_disks_transfer_grammar.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online-Grammar-Macro-SMDP Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (AG)",
   "language": "python",
   "name": "ag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
