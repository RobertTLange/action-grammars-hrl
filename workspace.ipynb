{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Action Grammars: A Grammar-Induction Based Method for Learning Temporal Abstractions\n",
    "## Authors: Robert Lange and Aldo Faisal | April 2019"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install -r requirements.txt --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:100% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:100% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "import gym\n",
    "import gym_hanoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agents.q_agent import Agent_Q\n",
    "from agents.smdp_q_agent import SMDP_Agent_Q, Macro, SMDPQTable\n",
    "# from agents.a2c_agent import ActorCritic, train_a2c_agent\n",
    "\n",
    "from learning.q_learning import  q_learning\n",
    "from learning.smdp_q_learning import smdp_q_learning, smdp_q_online_learning\n",
    "\n",
    "from learning.learning_params import *\n",
    "from learning.run_learning import *\n",
    "\n",
    "from utils.general import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directory - Learning performance\n",
    "results_dir = os.getcwd() + \"/results/\"\n",
    "if not os.path.exists(results_dir):\n",
    "    os.makedirs(results_dir)\n",
    "    print(\"Created New Results Directory\")\n",
    "\n",
    "# Create directory - Log directory\n",
    "log_dir = os.getcwd() + \"/logs/\"\n",
    "if not os.path.exists(log_dir):\n",
    "    os.makedirs(log_dir)\n",
    "    print(\"Created New Log Directory\")\n",
    "    \n",
    "# Create directory - Figure directory\n",
    "fig_dir = os.getcwd() + \"/figures/\"\n",
    "if not os.path.exists(fig_dir):\n",
    "    os.makedirs(fig_dir)\n",
    "    print(\"Created New Fig Directory\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Towers of Hanoi - Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.1, 'epsilon': 0.1}\n",
      "{'alpha': 0.8, 'gamma': 0.95, 'lambd': 0.0, 'epsilon': 0.1}\n"
     ]
    }
   ],
   "source": [
    "print(learning_parameters(\"Q-Learning\"))\n",
    "print(learning_parameters(\"Imitation-SMDP-Q-Learning\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_setup = {4: {\"num_episodes\": 300,\n",
    "                      \"max_steps\": 500},\n",
    "                  5: {\"num_episodes\": 1250,\n",
    "                      \"max_steps\": 1000},\n",
    "                  6: {\"num_episodes\": 5000,\n",
    "                      \"max_steps\": 2000},\n",
    "                  7: {\"num_episodes\": 10000,\n",
    "                      \"max_steps\": 4000},\n",
    "                  8: {\"num_episodes\": 20000,\n",
    "                      \"max_steps\": 8000}}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simple TD($\\lambda$) Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/rtl/anaconda2/envs/AG/lib/python3.6/site-packages/gym-0.10.9-py3.6.egg/gym/envs/registration.py:14: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
      "  result = entry_point.load(False)\n"
     ]
    }
   ],
   "source": [
    "# Setup for N=4 Disk Towers of Hanoi Environment\n",
    "log_episodes = 10\n",
    "log_freq = 50\n",
    "\n",
    "N = 6\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-da649fc6f0b7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mAgent_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mparams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlearning_parameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ml_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Q-Learning\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m hist, er_buffer = q_learning(env, agent, num_episodes, max_steps,\n\u001b[1;32m      4\u001b[0m                              \u001b[0;34m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m                              log_episodes=log_episodes, verbose=True)\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/agents/q_agent.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, env)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m         \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQTable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_movability_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/gym-hanoi/gym_hanoi/envs/hanoi_env.py\u001b[0m in \u001b[0;36mget_movability_map\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mdisks_from\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 138\u001b[0;31m                     \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisks_to\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdisks_from\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mdisks_to\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    139\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    140\u001b[0m                     \u001b[0mvalid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "agent = Agent_Q(env)\n",
    "params = learning_parameters(l_type=\"Q-Learning\")\n",
    "hist, er_buffer = q_learning(env, agent, num_episodes, max_steps,\n",
    "                             **params, log_freq=log_freq,\n",
    "                             log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Disks - Q-Learning: Run 1/5 Done - Time: 1.41\n",
      "4 Disks - Q-Learning: Run 2/5 Done - Time: 1.15\n",
      "4 Disks - Q-Learning: Run 3/5 Done - Time: 1.13\n",
      "4 Disks - Q-Learning: Run 4/5 Done - Time: 2.41\n",
      "4 Disks - Q-Learning: Run 5/5 Done - Time: 0.88\n",
      "Outfiled the results to results/4_disks_q.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_4_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/4_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Disks - Q-Learning: Run 1/5 Done - Time: 16.61\n",
      "5 Disks - Q-Learning: Run 2/5 Done - Time: 20.75\n",
      "5 Disks - Q-Learning: Run 3/5 Done - Time: 17.17\n",
      "5 Disks - Q-Learning: Run 4/5 Done - Time: 16.8\n",
      "5 Disks - Q-Learning: Run 5/5 Done - Time: 26.66\n",
      "Outfiled the results to results/5_disks_q.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_5_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/5_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_6_q = run_learning(\"Q-Learning\", num_times, num_disks,\n",
    "                                     num_episodes, max_steps,\n",
    "                                     log_episodes, log_freq,\n",
    "                                     save_fname=\"results/6_disks_q.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "deque([(0, (0, 0, 0, 0, 0), 1, (2, 0, 0, 0, 0)),\n",
       "       (1, (2, 0, 0, 0, 0), 0, (2, 1, 0, 0, 0)),\n",
       "       (2, (2, 1, 0, 0, 0), 5, (1, 1, 0, 0, 0)),\n",
       "       (3, (1, 1, 0, 0, 0), 1, (1, 1, 2, 0, 0)),\n",
       "       (4, (1, 1, 2, 0, 0), 2, (0, 1, 2, 0, 0)),\n",
       "       (5, (0, 1, 2, 0, 0), 3, (0, 2, 2, 0, 0)),\n",
       "       (6, (0, 2, 2, 0, 0), 1, (2, 2, 2, 0, 0)),\n",
       "       (7, (2, 2, 2, 0, 0), 0, (2, 2, 2, 1, 0)),\n",
       "       (8, (2, 2, 2, 1, 0), 5, (1, 2, 2, 1, 0)),\n",
       "       (9, (1, 2, 2, 1, 0), 4, (1, 0, 2, 1, 0)),\n",
       "       (10, (1, 0, 2, 1, 0), 2, (0, 0, 2, 1, 0)),\n",
       "       (11, (0, 0, 2, 1, 0), 5, (0, 0, 1, 1, 0)),\n",
       "       (12, (0, 0, 1, 1, 0), 1, (2, 0, 1, 1, 0)),\n",
       "       (13, (2, 0, 1, 1, 0), 0, (2, 1, 1, 1, 0)),\n",
       "       (14, (2, 1, 1, 1, 0), 5, (1, 1, 1, 1, 0)),\n",
       "       (15, (1, 1, 1, 1, 0), 1, (1, 1, 1, 1, 2)),\n",
       "       (16, (1, 1, 1, 1, 2), 2, (0, 1, 1, 1, 2)),\n",
       "       (17, (0, 1, 1, 1, 2), 3, (0, 2, 1, 1, 2)),\n",
       "       (18, (0, 2, 1, 1, 2), 1, (2, 2, 1, 1, 2)),\n",
       "       (19, (2, 2, 1, 1, 2), 2, (2, 2, 0, 1, 2)),\n",
       "       (20, (2, 2, 0, 1, 2), 4, (0, 2, 0, 1, 2)),\n",
       "       (21, (0, 2, 0, 1, 2), 5, (0, 1, 0, 1, 2)),\n",
       "       (22, (0, 1, 0, 1, 2), 1, (2, 1, 0, 1, 2)),\n",
       "       (23, (2, 1, 0, 1, 2), 2, (2, 0, 0, 1, 2)),\n",
       "       (24, (2, 0, 0, 1, 2), 4, (0, 0, 0, 1, 2)),\n",
       "       (25, (0, 0, 0, 1, 2), 3, (0, 0, 0, 2, 2)),\n",
       "       (26, (0, 0, 0, 2, 2), 0, (1, 0, 0, 2, 2)),\n",
       "       (27, (1, 0, 0, 2, 2), 1, (1, 2, 0, 2, 2)),\n",
       "       (28, (1, 2, 0, 2, 2), 3, (2, 2, 0, 2, 2)),\n",
       "       (29, (2, 2, 0, 2, 2), 0, (2, 2, 1, 2, 2)),\n",
       "       (30, (2, 2, 1, 2, 2), 5, (1, 2, 1, 2, 2)),\n",
       "       (31, (1, 2, 1, 2, 2), 4, (1, 0, 1, 2, 2)),\n",
       "       (32, (1, 0, 1, 2, 2), 2, (0, 0, 1, 2, 2)),\n",
       "       (33, (0, 0, 1, 2, 2), 3, (0, 0, 2, 2, 2)),\n",
       "       (34, (0, 0, 2, 2, 2), 0, (1, 0, 2, 2, 2)),\n",
       "       (35, (1, 0, 2, 2, 2), 1, (1, 2, 2, 2, 2)),\n",
       "       (36, (1, 2, 2, 2, 2), 3, (2, 2, 2, 2, 2))])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Return a greedy rollout Experience Replay Episode\n",
    "get_rollout_policy(env, agent, max_steps, grammar=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'bafbcdbafecfbafbcdbcefbcedabdafecdabd'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_rollout_policy(env, agent, max_steps, grammar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Context-Free Grammar Inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimal_policies = {4: \"abdaefabdcedabd\",\n",
    "                    5: \"bafbcdbafecfbafbcdbcfecdbafbcdb\",\n",
    "                    6: \"abdaefabdcedabdaefaedcefabdaefabdcedabdce\"\\\n",
    "                        \"faedcedabdaefabdcedabd\",\n",
    "                    7: \"bafbcdbafecfbafbcdbcfecdbafbcdbafecfbafec\"\\\n",
    "                        \"dbcfecfbafbcdbafecfbafbcdbcfecdbafbcdbcfe\"\\\n",
    "                        \"cfbafecdbcfecdbafbcdbafecfbafbcdbcfecdbafbcdb\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from grammars.cfg_grammar import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['1 a e f 1 c e d 1 \\\\n ', 'a b d ']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['abd']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "get_macros(\"all\", optimal_policies[4], 6, \"sequitur\", 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get_macros(\"all\", optimal_policies[4], 6, \"lexis\", 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imitation SMDP-Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 4\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 182.86/96.46 | Avg/Std Ret: 0.28/96.46 | Success R: 0.70\n",
      "Ep: 21 | Avg/Std Steps: 146.33/103.63 | Avg/Std Ret: 4.00/103.63 | Success R: 0.30\n",
      "Ep: 41 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 101 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 121 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 141 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 161 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 181 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 201 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 221 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 241 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 261 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n",
      "Ep: 281 | Avg/Std Steps: 17.00/0.00 | Avg/Std Ret: 44.01/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "macros = get_optimal_macros(env, N, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Imitation-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Disks - Imitation-SMDP-Q-Learning: Run 1/5 Done - Time: 1.3\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 2/5 Done - Time: 1.07\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 3/5 Done - Time: 1.12\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 4/5 Done - Time: 1.39\n",
      "4 Disks - Imitation-SMDP-Q-Learning: Run 5/5 Done - Time: 1.32\n",
      "Outfiled the results to results/4_disks_smdp_imi.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 4 Disk Environment\n",
    "num_times = 5\n",
    "num_disks = 4\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_4_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/4_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Disks - Imitation-SMDP-Q-Learning: Run 1/5 Done - Time: 23.63\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 2/5 Done - Time: 21.87\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 3/5 Done - Time: 23.22\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 4/5 Done - Time: 21.47\n",
      "5 Disks - Imitation-SMDP-Q-Learning: Run 5/5 Done - Time: 23.14\n",
      "Outfiled the results to results/5_disks_smdp_imi.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_5_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/5_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                   Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-94-67890b072b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m                                             \u001b[0mnum_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                             \u001b[0mlog_episodes\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlog_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                             save_fname=\"results/6_disks_smdp_imi.txt\")\n\u001b[0m",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/learning/run_learning.py\u001b[0m in \u001b[0;36mrun_learning\u001b[0;34m(l_type, num_times, num_disks, num_episodes, max_steps, log_episodes, log_freq, transfer_distance, save_fname)\u001b[0m\n\u001b[1;32m     40\u001b[0m                                               \u001b[0mlog_freq\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_freq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m                                               \u001b[0mlog_episodes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mlog_episodes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m                                               verbose=False)\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0ml_type\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"Transfer-SMDP-Q-Learning\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/learning/smdp_q_learning.py\u001b[0m in \u001b[0;36msmdp_q_learning\u001b[0;34m(env, agent, num_episodes, max_steps, gamma, alpha, lambd, epsilon, log_freq, log_episodes, verbose)\u001b[0m\n\u001b[1;32m     94\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mep_id\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0mlog_freq\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m             avg_steps, sd_steps, avg_ret, sd_ret, success_rate = greedy_eval(env, agent, gamma,\n\u001b[0;32m---> 96\u001b[0;31m                                                                              max_steps, log_episodes)\n\u001b[0m\u001b[1;32m     97\u001b[0m             hist[log_counter,:] = np.array([ep_id, avg_steps, sd_steps,\n\u001b[1;32m     98\u001b[0m                                             avg_ret, sd_ret, success_rate])\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/utils/general.py\u001b[0m in \u001b[0;36mgreedy_eval\u001b[0;34m(env, agent, gamma, max_steps, log_episodes)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mstp_temp\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mmax_steps\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcur_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0maction\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m5\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m                 next_state, reward, done, _ = macro_step(action, cur_state, agent,\n",
      "\u001b[0;32m~/Dropbox/PHD_ECN/PROJECTS/ActionGrammars/code/agents/q_agent.py\u001b[0m in \u001b[0;36mgreedy_action\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mgreedy_action\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mq_values\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwhere\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq_values\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 6 Disk Environment\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_6_smdp_imi = run_learning(\"Imitation-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                            num_episodes, max_steps,\n",
    "                                            log_episodes, log_freq,\n",
    "                                            save_fname=\"results/6_disks_smdp_imi.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Transfer Learning Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "N = 5\n",
    "num_episodes = learning_setup[N][\"num_episodes\"]\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(num_disks=N, env_noise=0, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 1017.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.10\n",
      "Ep: 21 | Avg/Std Steps: 309.50/18.50 | Avg/Std Ret: 0.00/18.50 | Success R: 0.20\n",
      "Ep: 41 | Avg/Std Steps: 683.67/370.92 | Avg/Std Ret: 0.01/370.92 | Success R: 0.30\n",
      "Ep: 61 | Avg/Std Steps: 1000.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 81 | Avg/Std Steps: 344.75/138.37 | Avg/Std Ret: 0.00/138.37 | Success R: 0.40\n",
      "Ep: 101 | Avg/Std Steps: 127.00/58.70 | Avg/Std Ret: 1.31/58.70 | Success R: 0.70\n",
      "Ep: 121 | Avg/Std Steps: 60.50/19.41 | Avg/Std Ret: 6.48/19.41 | Success R: 1.00\n",
      "Ep: 141 | Avg/Std Steps: 49.00/0.00 | Avg/Std Ret: 8.53/0.00 | Success R: 1.00\n",
      "Ep: 161 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 181 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 201 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 221 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 241 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 261 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 281 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 301 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 321 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 341 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 361 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 381 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 401 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 421 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 441 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 461 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 481 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 501 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 521 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 541 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 561 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 581 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 601 | Avg/Std Steps: 47.00/0.00 | Avg/Std Ret: 9.45/0.00 | Success R: 1.00\n",
      "Ep: 621 | Avg/Std Steps: 46.00/0.00 | Avg/Std Ret: 9.94/0.00 | Success R: 1.00\n",
      "Ep: 641 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 661 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 681 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 701 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 721 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 741 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 761 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 781 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 801 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 821 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 841 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 861 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 881 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 901 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 921 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 941 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 961 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 981 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1001 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1021 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1041 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1061 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1081 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1101 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1121 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1141 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1161 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1181 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1201 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1221 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n",
      "Ep: 1241 | Avg/Std Steps: 45.00/0.00 | Avg/Std Ret: 10.47/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "macros = get_optimal_macros(env, N-1, \"Sequitur\")\n",
    "agent = SMDP_Agent_Q(env, macros)\n",
    "params = learning_parameters(l_type=\"Transfer-SMDP-Q-Learning\")\n",
    "hist, er_buffer = smdp_q_learning(env, agent, num_episodes, max_steps,\n",
    "                                  **params,\n",
    "                                  log_freq=log_freq,\n",
    "                                  log_episodes=log_episodes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Disks - Transfer-SMDP-Q-Learning: Run 1/5 Done - Time: 24.96\n",
      "5 Disks - Transfer-SMDP-Q-Learning: Run 2/5 Done - Time: 24.05\n",
      "5 Disks - Transfer-SMDP-Q-Learning: Run 3/5 Done - Time: 25.51\n",
      "5 Disks - Transfer-SMDP-Q-Learning: Run 4/5 Done - Time: 25.25\n",
      "5 Disks - Transfer-SMDP-Q-Learning: Run 5/5 Done - Time: 27.24\n",
      "Outfiled the results to results/5_disks_smdp_transfer_4_disks.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment with 4 disk grammar\n",
    "num_times = 5\n",
    "num_disks = 5\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "env, agent, stats_smdp_trans_5 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                              num_episodes, max_steps,\n",
    "                                              log_episodes, log_freq,\n",
    "                                              transfer_distance=1,\n",
    "                                              save_fname=\"results/5_disks_smdp_transfer_4_disks.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_times = 5\n",
    "num_disks = 6\n",
    "num_episodes = learning_setup[num_disks][\"num_episodes\"]\n",
    "max_steps = learning_setup[num_disks][\"max_steps\"]\n",
    "\n",
    "# Run Learning 5 times for 6 Disk Environment with 5 disk grammar\n",
    "env, agent, stats_smdp_trans_5 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                              num_episodes, max_steps,\n",
    "                                              log_episodes, log_freq,\n",
    "                                              transfer_distance=1,\n",
    "                                              save_fname=\"results/6_disks_smdp_transfer_5_disks.txt\")\n",
    "\n",
    "# Run Learning 5 times for 6 Disk Environment with 4 disk grammar\n",
    "env, agent, stats_smdp_trans_4 = run_learning(\"Transfer-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                              num_episodes, max_steps,\n",
    "                                              log_episodes, log_freq,\n",
    "                                              transfer_distance=2,\n",
    "                                              save_fname=\"results/6_disks_smdp_transfer_4_disks.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Online-Grammar-Macro-SMDP Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ep:  1 | Avg/Std Steps: 177.33/144.50 | Avg/Std Ret: 1.88/144.50 | Success R: 0.30\n",
      "Ep:  1 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 21 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 41 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep:  1 | Avg/Std Steps: 231.14/110.41 | Avg/Std Ret: 0.16/110.41 | Success R: 0.70\n",
      "Ep: 21 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 41 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 20.00/0.00 | Avg/Std Ret: 37.74/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 19.00/0.00 | Avg/Std Ret: 39.72/0.00 | Success R: 1.00\n",
      "Ep:  1 | Avg/Std Steps: 241.00/151.92 | Avg/Std Ret: 1.37/151.92 | Success R: 0.70\n",
      "Ep: 21 | Avg/Std Steps: 500.00/0.00 | Avg/Std Ret: 0.00/0.00 | Success R: 0.00\n",
      "Ep: 41 | Avg/Std Steps: 16.00/0.00 | Avg/Std Ret: 46.33/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 16.00/0.00 | Avg/Std Ret: 46.33/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n",
      "Ep:  1 | Avg/Std Steps: 170.67/141.19 | Avg/Std Ret: 2.09/141.19 | Success R: 0.30\n",
      "Ep: 21 | Avg/Std Steps: 18.00/0.00 | Avg/Std Ret: 41.81/0.00 | Success R: 1.00\n",
      "Ep: 41 | Avg/Std Steps: 16.00/0.00 | Avg/Std Ret: 46.33/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n",
      "Ep:  1 | Avg/Std Steps: 143.00/39.07 | Avg/Std Ret: 0.29/39.07 | Success R: 0.50\n",
      "Ep: 21 | Avg/Std Steps: 132.38/143.11 | Avg/Std Ret: 12.18/143.11 | Success R: 0.80\n",
      "Ep: 41 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n",
      "Ep: 61 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n",
      "Ep: 81 | Avg/Std Steps: 15.00/0.00 | Avg/Std Ret: 48.77/0.00 | Success R: 1.00\n"
     ]
    }
   ],
   "source": [
    "# Setup for N=4 Disk Towers of Hanoi Environment\n",
    "log_episodes = 10\n",
    "log_freq = 20\n",
    "\n",
    "init_q_eps = 20\n",
    "inter_update_eps=100\n",
    "num_grammar_updates=5\n",
    "\n",
    "N = 4\n",
    "max_steps = learning_setup[N][\"max_steps\"]\n",
    "\n",
    "env = gym.make(\"Hanoi-v0\")\n",
    "env.set_env_parameters(N, env_noise=0, verbose=False)\n",
    "\n",
    "params = learning_parameters(l_type=\"Q-Learning\")\n",
    "hist = smdp_q_online_learning(env, init_q_eps, inter_update_eps,\n",
    "                              num_grammar_updates, max_steps, **params,\n",
    "                              log_freq=log_freq, log_episodes=log_episodes,\n",
    "                              verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Disks - Online-SMDP-Q-Learning: Run 1/5 Done - Time: 72.96\n",
      "4 Disks - Online-SMDP-Q-Learning: Run 2/5 Done - Time: 58.4\n",
      "4 Disks - Online-SMDP-Q-Learning: Run 3/5 Done - Time: 22.89\n",
      "4 Disks - Online-SMDP-Q-Learning: Run 4/5 Done - Time: 60.61\n",
      "4 Disks - Online-SMDP-Q-Learning: Run 5/5 Done - Time: 66.61\n",
      "Outfiled the results to results/4_disks_smdp_online_no_transfer.txt.\n"
     ]
    }
   ],
   "source": [
    "# Run Learning 5 times for 5 Disk Environment\n",
    "num_disks = 4\n",
    "stats_smdp_online = run_learning(\"Online-SMDP-Q-Learning\", num_times, num_disks,\n",
    "                                 num_episodes, max_steps,\n",
    "                                 log_episodes, log_freq,\n",
    "                                 save_fname=\"results/4_disks_smdp_online_no_transfer.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3 (AG)",
   "language": "python",
   "name": "ag"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
